{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Softmax Model with 1 hidden Layer\n",
    "\n",
    "MNIST is a great dataset to use to write a simple NN.  MNIST is a dataset of handwritten digits.  The handwritten digits are represented by a 28 by 28 pixel matrix, with each pixel ranging from 0-255, the darkness of that pixel.  Let's see if we can obtain any accuracy in predicting handwritten digit by using each pixel as a parameter.\n",
    "\n",
    "We are going to load some MNIST training data into a NN.  Run it through 1 hidden layer using the sigmoid function.  Finally we are going to run the output layer through the softmax function to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's throw away the vertical pixel adjacency data by flattening the 28x28 pixel image into a vector of length 28x28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PIXELS = 28*28\n",
    "\n",
    "def normalize_input(x):\n",
    "    flattened = np.reshape(x, (x.shape[0], NUM_PIXELS))\n",
    "    stddev = flattened.std(axis=0) + 1e-100\n",
    "    mean = flattened.mean(axis=0)\n",
    "    return (flattened - mean) / stddev\n",
    "\n",
    "# by subtracting the mean, and dividing by the standard deviation, \n",
    "# we attempt to coerce our inputs into a normal distribution. Although the method is not perfect, this should give \n",
    "# us input variance closer to 1.  \n",
    "# Later on, when we feed Z values through the sigmoid function, we would like those values to also have variance of 1.\n",
    "# Reason being that the derivative at those specific points in the line will be slanted, and we can acheive some success \n",
    "# In moving in the correct directions when we take one step of stochastic gradient descent.\n",
    "\n",
    "# In the alternate scenatio, if we feed very large values through the squasher function, we will end up with derivatives\n",
    "# that are near 0.  In such a case, one step of gradient descent will not help us as much.  \n",
    "\n",
    "# product of variances  var_a * var_b = var_c\n",
    "# variance of the sum of normally distributed weights = sum of the variances.  so 1/#inputs should be the variance of our weights. \n",
    "# ... at least to start.\n",
    "\n",
    "new_x_train = normalize_input(x_train)\n",
    "new_x_test = normalize_input(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    X = tf.placeholder(shape=[None, NUM_PIXELS], dtype=tf.float32, name=\"input\")\n",
    "    y = tf.placeholder(shape=[None], dtype=tf.int32, name=\"y\")\n",
    "    Y = tf.one_hot(y, 10)\n",
    "    learning_rate = tf.placeholder(shape=[], dtype=tf.float32, name=\"learning_rate\")\n",
    "    \n",
    "    #First layer \n",
    "    W1 = tf.Variable(\n",
    "        np.random.normal(size = [NUM_PIXELS, 128], scale = 1/np.sqrt(NUM_PIXELS)),\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    b1 = tf.Variable(np.zeros(shape = [128]), dtype=tf.float32)\n",
    "    \n",
    "    Z1 = tf.matmul(X, W1) + b1\n",
    "    H1 = tf.nn.sigmoid(Z1)\n",
    "    \n",
    "    #Second Layer\n",
    "    W2 = tf.Variable(\n",
    "        np.random.normal(size = [128, 10], scale = 1/np.sqrt(128)),\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    b2 = tf.Variable(np.zeros(shape = [10]), dtype=tf.float32)\n",
    "    Z2 = tf.matmul(H1, W2) + b2 \n",
    "    H2 = tf.nn.softmax(Z2) \n",
    "    \n",
    "    #Calculating Error, Mean Cross Entropy\n",
    "    negative_logs = -tf.log(H2)\n",
    "    errors = tf.reduce_sum(Y * negative_logs, axis = 1) \n",
    "    mean_error = tf.reduce_mean(errors) \n",
    "    \n",
    "    predictions = tf.argmax(H2, axis = 1, output_type = tf.int32)\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(y, predictions), tf.float32)\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    optimizer = tf.train.GradientDescentOptimizer(\n",
    "        learning_rate = learning_rate\n",
    "    )\n",
    "    train_step = optimizer.minimize(mean_error)\n",
    "    \n",
    "    return {\n",
    "        \"X\": X, \n",
    "        \"y\": y,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"mean_error\": mean_error,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"train_step\": train_step,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our batches from x and y\n",
    "BATCH_SIZE = 32\n",
    "mnist_batches = []\n",
    "\n",
    "for batch_start in range(0, new_x_train.shape[0], BATCH_SIZE):\n",
    "    batch_x = new_x_train[batch_start:batch_start + BATCH_SIZE, :]\n",
    "    batch_y = y_train[batch_start:batch_start + BATCH_SIZE]\n",
    "    \n",
    "    mnist_batches.append((batch_x, batch_y))\n",
    "    # store batches in tuples in mnist_batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: 0 | ME: 2.36 | ACC: 0.13\n",
      "B: 100 | ME: 1.91 | ACC: 0.59\n",
      "B: 200 | ME: 1.60 | ACC: 0.68\n",
      "B: 300 | ME: 1.38 | ACC: 0.74\n",
      "B: 400 | ME: 1.20 | ACC: 0.78\n",
      "B: 500 | ME: 1.07 | ACC: 0.80\n",
      "B: 600 | ME: 0.96 | ACC: 0.82\n",
      "B: 700 | ME: 0.87 | ACC: 0.84\n",
      "B: 800 | ME: 0.81 | ACC: 0.84\n",
      "B: 900 | ME: 0.75 | ACC: 0.85\n",
      "B: 1000 | ME: 0.70 | ACC: 0.86\n",
      "B: 1100 | ME: 0.66 | ACC: 0.86\n",
      "B: 1200 | ME: 0.63 | ACC: 0.86\n",
      "B: 1300 | ME: 0.60 | ACC: 0.87\n",
      "B: 1400 | ME: 0.58 | ACC: 0.87\n",
      "B: 1500 | ME: 0.56 | ACC: 0.87\n",
      "B: 1600 | ME: 0.54 | ACC: 0.88\n",
      "B: 1700 | ME: 0.52 | ACC: 0.88\n",
      "B: 1800 | ME: 0.50 | ACC: 0.88\n",
      "B: 0 | ME: 0.49 | ACC: 0.88\n",
      "B: 100 | ME: 0.48 | ACC: 0.88\n",
      "B: 200 | ME: 0.47 | ACC: 0.88\n",
      "B: 300 | ME: 0.46 | ACC: 0.89\n",
      "B: 400 | ME: 0.45 | ACC: 0.89\n",
      "B: 500 | ME: 0.44 | ACC: 0.89\n",
      "B: 600 | ME: 0.43 | ACC: 0.89\n",
      "B: 700 | ME: 0.42 | ACC: 0.89\n",
      "B: 800 | ME: 0.42 | ACC: 0.89\n",
      "B: 900 | ME: 0.41 | ACC: 0.90\n",
      "B: 1000 | ME: 0.40 | ACC: 0.90\n",
      "B: 1100 | ME: 0.40 | ACC: 0.90\n",
      "B: 1200 | ME: 0.39 | ACC: 0.90\n",
      "B: 1300 | ME: 0.39 | ACC: 0.90\n",
      "B: 1400 | ME: 0.38 | ACC: 0.90\n",
      "B: 1500 | ME: 0.38 | ACC: 0.90\n",
      "B: 1600 | ME: 0.37 | ACC: 0.90\n",
      "B: 1700 | ME: 0.37 | ACC: 0.90\n",
      "B: 1800 | ME: 0.37 | ACC: 0.90\n",
      "B: 0 | ME: 0.36 | ACC: 0.90\n",
      "B: 100 | ME: 0.36 | ACC: 0.90\n",
      "B: 200 | ME: 0.36 | ACC: 0.90\n",
      "B: 300 | ME: 0.36 | ACC: 0.90\n",
      "B: 400 | ME: 0.35 | ACC: 0.90\n",
      "B: 500 | ME: 0.35 | ACC: 0.90\n",
      "B: 600 | ME: 0.35 | ACC: 0.91\n",
      "B: 700 | ME: 0.34 | ACC: 0.91\n",
      "B: 800 | ME: 0.34 | ACC: 0.91\n",
      "B: 900 | ME: 0.34 | ACC: 0.91\n",
      "B: 1000 | ME: 0.33 | ACC: 0.91\n",
      "B: 1100 | ME: 0.33 | ACC: 0.91\n",
      "B: 1200 | ME: 0.33 | ACC: 0.91\n",
      "B: 1300 | ME: 0.33 | ACC: 0.91\n",
      "B: 1400 | ME: 0.33 | ACC: 0.91\n",
      "B: 1500 | ME: 0.32 | ACC: 0.91\n",
      "B: 1600 | ME: 0.32 | ACC: 0.91\n",
      "B: 1700 | ME: 0.32 | ACC: 0.91\n",
      "B: 1800 | ME: 0.32 | ACC: 0.91\n",
      "B: 0 | ME: 0.32 | ACC: 0.91\n",
      "B: 100 | ME: 0.32 | ACC: 0.91\n",
      "B: 200 | ME: 0.31 | ACC: 0.91\n",
      "B: 300 | ME: 0.31 | ACC: 0.91\n",
      "B: 400 | ME: 0.31 | ACC: 0.91\n",
      "B: 500 | ME: 0.31 | ACC: 0.91\n",
      "B: 600 | ME: 0.31 | ACC: 0.91\n",
      "B: 700 | ME: 0.31 | ACC: 0.91\n",
      "B: 800 | ME: 0.31 | ACC: 0.91\n",
      "B: 900 | ME: 0.30 | ACC: 0.91\n",
      "B: 1000 | ME: 0.30 | ACC: 0.91\n",
      "B: 1100 | ME: 0.30 | ACC: 0.91\n",
      "B: 1200 | ME: 0.30 | ACC: 0.91\n",
      "B: 1300 | ME: 0.30 | ACC: 0.91\n",
      "B: 1400 | ME: 0.30 | ACC: 0.91\n",
      "B: 1500 | ME: 0.30 | ACC: 0.92\n",
      "B: 1600 | ME: 0.30 | ACC: 0.91\n",
      "B: 1700 | ME: 0.29 | ACC: 0.91\n",
      "B: 1800 | ME: 0.29 | ACC: 0.92\n",
      "B: 0 | ME: 0.29 | ACC: 0.92\n",
      "B: 100 | ME: 0.29 | ACC: 0.92\n",
      "B: 200 | ME: 0.29 | ACC: 0.92\n",
      "B: 300 | ME: 0.29 | ACC: 0.92\n",
      "B: 400 | ME: 0.29 | ACC: 0.92\n",
      "B: 500 | ME: 0.29 | ACC: 0.92\n",
      "B: 600 | ME: 0.29 | ACC: 0.92\n",
      "B: 700 | ME: 0.29 | ACC: 0.92\n",
      "B: 800 | ME: 0.29 | ACC: 0.92\n",
      "B: 900 | ME: 0.28 | ACC: 0.92\n",
      "B: 1000 | ME: 0.28 | ACC: 0.92\n",
      "B: 1100 | ME: 0.28 | ACC: 0.92\n",
      "B: 1200 | ME: 0.28 | ACC: 0.92\n",
      "B: 1300 | ME: 0.28 | ACC: 0.92\n",
      "B: 1400 | ME: 0.28 | ACC: 0.92\n",
      "B: 1500 | ME: 0.28 | ACC: 0.92\n",
      "B: 1600 | ME: 0.28 | ACC: 0.92\n",
      "B: 1700 | ME: 0.28 | ACC: 0.92\n",
      "B: 1800 | ME: 0.28 | ACC: 0.92\n",
      "B: 0 | ME: 0.28 | ACC: 0.92\n",
      "B: 100 | ME: 0.28 | ACC: 0.92\n",
      "B: 200 | ME: 0.27 | ACC: 0.92\n",
      "B: 300 | ME: 0.28 | ACC: 0.92\n",
      "B: 400 | ME: 0.27 | ACC: 0.92\n",
      "B: 500 | ME: 0.27 | ACC: 0.92\n",
      "B: 600 | ME: 0.27 | ACC: 0.92\n",
      "B: 700 | ME: 0.27 | ACC: 0.92\n",
      "B: 800 | ME: 0.27 | ACC: 0.92\n",
      "B: 900 | ME: 0.27 | ACC: 0.92\n",
      "B: 1000 | ME: 0.27 | ACC: 0.92\n",
      "B: 1100 | ME: 0.27 | ACC: 0.92\n",
      "B: 1200 | ME: 0.27 | ACC: 0.92\n",
      "B: 1300 | ME: 0.27 | ACC: 0.92\n",
      "B: 1400 | ME: 0.27 | ACC: 0.92\n",
      "B: 1500 | ME: 0.27 | ACC: 0.92\n",
      "B: 1600 | ME: 0.27 | ACC: 0.92\n",
      "B: 1700 | ME: 0.27 | ACC: 0.92\n",
      "B: 1800 | ME: 0.27 | ACC: 0.92\n",
      "B: 0 | ME: 0.26 | ACC: 0.92\n",
      "B: 100 | ME: 0.26 | ACC: 0.92\n",
      "B: 200 | ME: 0.26 | ACC: 0.92\n",
      "B: 300 | ME: 0.26 | ACC: 0.92\n",
      "B: 400 | ME: 0.26 | ACC: 0.92\n",
      "B: 500 | ME: 0.26 | ACC: 0.93\n",
      "B: 600 | ME: 0.26 | ACC: 0.92\n",
      "B: 700 | ME: 0.26 | ACC: 0.92\n",
      "B: 800 | ME: 0.26 | ACC: 0.92\n",
      "B: 900 | ME: 0.26 | ACC: 0.93\n",
      "B: 1000 | ME: 0.26 | ACC: 0.93\n",
      "B: 1100 | ME: 0.26 | ACC: 0.93\n",
      "B: 1200 | ME: 0.26 | ACC: 0.93\n",
      "B: 1300 | ME: 0.26 | ACC: 0.93\n",
      "B: 1400 | ME: 0.26 | ACC: 0.93\n",
      "B: 1500 | ME: 0.26 | ACC: 0.93\n",
      "B: 1600 | ME: 0.26 | ACC: 0.93\n",
      "B: 1700 | ME: 0.26 | ACC: 0.93\n",
      "B: 1800 | ME: 0.26 | ACC: 0.93\n",
      "B: 0 | ME: 0.25 | ACC: 0.93\n",
      "B: 100 | ME: 0.25 | ACC: 0.93\n",
      "B: 200 | ME: 0.25 | ACC: 0.93\n",
      "B: 300 | ME: 0.25 | ACC: 0.93\n",
      "B: 400 | ME: 0.25 | ACC: 0.93\n",
      "B: 500 | ME: 0.25 | ACC: 0.93\n",
      "B: 600 | ME: 0.25 | ACC: 0.93\n",
      "B: 700 | ME: 0.25 | ACC: 0.93\n",
      "B: 800 | ME: 0.25 | ACC: 0.93\n",
      "B: 900 | ME: 0.25 | ACC: 0.93\n",
      "B: 1000 | ME: 0.25 | ACC: 0.93\n",
      "B: 1100 | ME: 0.25 | ACC: 0.93\n",
      "B: 1200 | ME: 0.25 | ACC: 0.93\n",
      "B: 1300 | ME: 0.25 | ACC: 0.93\n",
      "B: 1400 | ME: 0.25 | ACC: 0.93\n",
      "B: 1500 | ME: 0.25 | ACC: 0.93\n",
      "B: 1600 | ME: 0.25 | ACC: 0.93\n",
      "B: 1700 | ME: 0.25 | ACC: 0.93\n",
      "B: 1800 | ME: 0.25 | ACC: 0.93\n",
      "B: 0 | ME: 0.25 | ACC: 0.93\n",
      "B: 100 | ME: 0.25 | ACC: 0.93\n",
      "B: 200 | ME: 0.25 | ACC: 0.93\n",
      "B: 300 | ME: 0.25 | ACC: 0.93\n",
      "B: 400 | ME: 0.25 | ACC: 0.93\n",
      "B: 500 | ME: 0.24 | ACC: 0.93\n",
      "B: 600 | ME: 0.24 | ACC: 0.93\n",
      "B: 700 | ME: 0.24 | ACC: 0.93\n",
      "B: 800 | ME: 0.24 | ACC: 0.93\n",
      "B: 900 | ME: 0.24 | ACC: 0.93\n",
      "B: 1000 | ME: 0.24 | ACC: 0.93\n",
      "B: 1100 | ME: 0.24 | ACC: 0.93\n",
      "B: 1200 | ME: 0.24 | ACC: 0.93\n",
      "B: 1300 | ME: 0.24 | ACC: 0.93\n",
      "B: 1400 | ME: 0.24 | ACC: 0.93\n",
      "B: 1500 | ME: 0.24 | ACC: 0.93\n",
      "B: 1600 | ME: 0.24 | ACC: 0.93\n",
      "B: 1700 | ME: 0.24 | ACC: 0.93\n",
      "B: 1800 | ME: 0.24 | ACC: 0.93\n",
      "B: 0 | ME: 0.24 | ACC: 0.93\n",
      "B: 100 | ME: 0.24 | ACC: 0.93\n",
      "B: 200 | ME: 0.24 | ACC: 0.93\n",
      "B: 300 | ME: 0.24 | ACC: 0.93\n",
      "B: 400 | ME: 0.24 | ACC: 0.93\n",
      "B: 500 | ME: 0.24 | ACC: 0.93\n",
      "B: 600 | ME: 0.24 | ACC: 0.93\n",
      "B: 700 | ME: 0.24 | ACC: 0.93\n",
      "B: 800 | ME: 0.24 | ACC: 0.93\n",
      "B: 900 | ME: 0.24 | ACC: 0.93\n",
      "B: 1000 | ME: 0.24 | ACC: 0.93\n",
      "B: 1100 | ME: 0.24 | ACC: 0.93\n",
      "B: 1200 | ME: 0.24 | ACC: 0.93\n",
      "B: 1300 | ME: 0.24 | ACC: 0.93\n",
      "B: 1400 | ME: 0.24 | ACC: 0.93\n",
      "B: 1500 | ME: 0.24 | ACC: 0.93\n",
      "B: 1600 | ME: 0.23 | ACC: 0.93\n",
      "B: 1700 | ME: 0.23 | ACC: 0.93\n",
      "B: 1800 | ME: 0.23 | ACC: 0.93\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "\n",
    "def train_batch(session, batch_x, batch_y, graph):\n",
    "    session.run(\n",
    "        graph[\"train_step\"],\n",
    "        feed_dict = {\n",
    "            graph[\"X\"]: batch_x,\n",
    "            graph[\"y\"]: batch_y,\n",
    "            graph[\"learning_rate\"]: LEARNING_RATE\n",
    "        }\n",
    "    )\n",
    "    \n",
    "def evaluate_model(batch_idx, session, graph):\n",
    "    me, acc = session.run(\n",
    "        [graph[\"mean_error\"], graph[\"accuracy\"]],\n",
    "        feed_dict = {\n",
    "            graph[\"X\"]: new_x_test,\n",
    "            graph[\"y\"]: y_test,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f'B: {batch_idx} | ME: {me:0.2f} | ACC: {acc:0.2f}')\n",
    "\n",
    "def train_epoch(session, graph):\n",
    "    for (batch_idx, (batch_x, batch_y)) in enumerate(mnist_batches):\n",
    "        train_batch(\n",
    "            session = session,\n",
    "            batch_x = batch_x,\n",
    "            batch_y = batch_y,\n",
    "            graph = graph\n",
    "        )\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            evaluate_model(batch_idx, session, graph)\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "with tf.Session() as session:\n",
    "    graph = build_graph()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_idx in range(NUM_EPOCHS):\n",
    "        train_epoch(session, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to achieve 94% accuracy in predicting handwritten digits after about 10 epochs.  Notice the help we got in speed for calculating our data by normalizing it to start.  89% accuracy on the test set after the first epoch as opposed to 80% if we had just pushed inputs to range 0-1.  Far worse than that I assume if we had not normalized at all.  Well we would have just needed to adjust learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
