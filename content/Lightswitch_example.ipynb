{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear relationships with NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are simply multiple layers of logistic regressions.  They are useful in uncovering non-linear relationships.  They are useful in cases where one parameter alone may not contain predictive power, however the value of one variable, based on if another variable is present.  The conditional probability.  https://stats.stackexchange.com/a/41290.\n",
    "\n",
    "Let's take a practical example of two lightswitches and a light.  Where the output, or rather, the light being on depends on both switches.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice figure 1.0 below.  We will denote the position of lightswitch 1 as $X_{1}$, the position of lighswitch 2 as $X_{2}$, and then whether or not the light is on by the output classifier, $Y$.\n",
    "\n",
    "The light on if only 1 of the lights is switched on.  If they are both on, the effect is negated and the light turns off.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Figure 1.0: Lightswitch example\n",
    "\n",
    "$X_{1}$ | $X_{2}$ | $Y$\n",
    ":--- | --- | ---\n",
    "0   | 0   | 0\n",
    "1   | 0   | 1\n",
    "0   | 1   | 1\n",
    "1   | 1   | 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's the problem?  Why can't we describe this relationship using a logistic regression.  Well the issue is that these variables are not linearly correlated.  More specifically, the value of one variable doesn't give us any information about the output.  They are dependent on eachother.  Plotting $X_{1}$ against $X_{2}$ on a plane, will visually show us why this is the case.  Notice Plot 1.0 below.\n",
    "\n",
    "It would be impossible to draw a linear decision boundary that correctly seperates the negative outputs from the positives.  As an example, I have drawn $X_{2} = X_{1} * 0.6 + 0.2$ But any way we slice the plane, we are isolating 1 correct and 1 incorrect training example.  This is the first hint that we need either a more complicated decision boundary, or a more powerful method.   Namely, Neural Networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,1.2,'Plot 1.0: X_1 against X_2')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([0, 1], [1, 0], 'go')\n",
    "plt.plot([0, 1], [0, 1], 'rx')\n",
    "decision_boundary_xis = np.arange(0, 1, 0.01)\n",
    "plt.plot(decision_boundary_xis, decision_boundary_xis * .6 + 0.2, '.')\n",
    "\n",
    "\n",
    "plt.xlabel('Lightswitch 1')\n",
    "plt.ylabel('Lightswitch 2')\n",
    "plt.text(0,1.2, 'Plot 1.0: X_1 against X_2', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With neural networks, we add layers of abstraction.  Layers of more logistic regressions, to apply more complicated relationships to the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niko/anaconda3/envs/ned/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/niko/anaconda3/envs/ned/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lightswitch](https://res.cloudinary.com/dlpclqzwk/image/upload/v1517370192/lightswitch_hrvuoq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above tensorflow graph, I have drawn out the classifier portion.  An input X feeds through a first layer of logistic regressions.  After passing outputs through the sigmoid squasher function, the aforenamed hidden activations are passed through a second layer of logistic regressions to give us output of the prediction of if the light is on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niko/anaconda3/envs/ned/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# our inputs.\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "])\n",
    "\n",
    "Y = np.array([\n",
    "    0, 1, 1, 0\n",
    "])\n",
    "\n",
    "w1 = np.array([\n",
    "    [1000, -1000],\n",
    "    [-1000, 1000],\n",
    "])\n",
    "b1 = -500\n",
    "# calculate the hidden activations for layer 1.  \n",
    "\n",
    "theta_x = X.dot(w1)\n",
    "z_1 = theta_x + b1\n",
    "ha_1 = sigmoid(z_1)\n",
    "\n",
    "# Begin Layer 2\n",
    "\n",
    "w2 = np.array([1000, 1000])\n",
    "b2 = -500\n",
    "\n",
    "z_2 = ha_1.dot(w2) + b2\n",
    "z_2\n",
    "output = sigmoid(z_2)\n",
    "\n",
    "np.around(output) # output predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we are getting the same values as our Y, training array above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
