{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting IMDB Sentiment purely from words in review \n",
    "\n",
    "**Data**:  Data is from IMDB, neatly stored in a keras dataset.  IMDB provides data from 25_000 reviews and the corresponding result for whether the review is positive or negative.\n",
    "\n",
    "**Hypothesis**: We can predict the sentiment of a review from a list of 1_000 of the most frequent words used in the reviews as a whole, and whether or not the word exists in a review.  \n",
    "\n",
    "**Definitions**\n",
    "Review:  A review's is the words of the review, represented as numbers in a numpy array.  The number indicates the frequency of the word in the dataset as a whole.  Smaller numbers are more frequent.\n",
    "\n",
    "**Parameters/features/x_values** Variables we will use to predict the outcome.\n",
    "\n",
    "**Supervised learning**: We will have the 'right answers' to the review's sentiment.  Y values will be either 0's or 1's indicating if the review was actually negative or positive respectively.  These discrete output value makes this a classification problem, which is great for logistic regression.\n",
    "\n",
    "As features we will use whether or not a word is included in a review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-42fe4f9b921d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTOP_N_WORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1_000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# To speed up model, we will only analyze the most frequent TOP_N_WORDS words in all reviews.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "\n",
    "TOP_N_WORDS = 1_000\n",
    "# To speed up model, we will only analyze the most frequent TOP_N_WORDS words in all reviews.  \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=TOP_N_WORDS,\n",
    "                                                      )\n",
    "\n",
    "# x_train is our training data.  We would like to use x_train to predict y_train.  Review data and\n",
    "# corresponding sentiment.  \n",
    "# x_test is the test data.  Used to predict y_test values.  We can use this to test our model at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example the first training review has len(x_train[0]) many words.\n",
    "print('The first training review has {0} many words and is a {1} review.\\n 0 for negative, 1 for positive'.format(len(x_train[0]), y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorizing each review.\n",
    "# We will sacrifice word order, word frequency in individual reviews, and word pairs for simplicity.\n",
    "# Create a word vector with length TOP_N_WORDS + 1, holding 0s or 1s if the word exists in the review,\n",
    "# aka our parameters\n",
    "\n",
    "# + 1 is added for the y intercept parameter.  This simplifies vector multiplication later on.\n",
    "\n",
    "def word_vector(review):\n",
    "    result = np.zeros(TOP_N_WORDS + 1)\n",
    "    for word_index in review:\n",
    "        result[word_index] = 1\n",
    "    result[0] = 1\n",
    "    return result\n",
    "    \n",
    "def vectorize_data(x_is):\n",
    "    return np.array([word_vector(review) for review in x_is])\n",
    "\n",
    "new_x_train = vectorize_data(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thetas = np.zeros(TOP_N_WORDS + 1)  \n",
    "\n",
    "# initialize thetas for each parameter at 0.  The predictive weight of each parameter, to tell us\n",
    "# whether the model thinks the review is positive or negative.\n",
    "\n",
    "# For example, if the 100th most frequent word in the dataset is the word 'good', then\n",
    "# theta_100 is the predictive power for the word good, if it turns out to be higher than theta of other words, \n",
    "# That means that 'good' being present in a review has a higher likelihood that the review is positive.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will feed our parameters for each review through the sigmoid function, a squasher function to \n",
    "# calculate predict sentiment for each review based on our thetas.  Outputs will be between 0 and 1 for each review.\n",
    "\n",
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))  # 0 < output < 1\n",
    "\n",
    "def calc_predictions(x_is, thetas):\n",
    "    return sigma(x_is.dot(thetas))\n",
    "\n",
    "predictions = calc_predictions(new_x_train, thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, ys): # faster with a list comprehension?\n",
    "    predict_was_right = [(prediction > 0.5) == ys[i] for i, prediction in enumerate(predictions)]\n",
    "    return sum(predict_was_right) / ys.shape[0]\n",
    "\n",
    "acc = accuracy(predictions, y_train)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first guess at the thetas turned out to be 50%.  Which seems appropriate so far because we guessed 0 for all thetas.  We guessed that each of the TOP_N_WORDS existing in individual reviews had no predictive power.  \n",
    "\n",
    "Now we can use a learning rate combined with subtracting the derivate of the cost function to adjust thetas up and doen respectively based on the derivative..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the function we will use to calculate the error for the model.\n",
    "\n",
    "\\begin{equation*}\n",
    "CE = \\frac{1}{m} * \\sum_{i=1}^m [ -y^i * log(h_{ \\theta}(x^i)) - (1 - y^i) * log(1 - h_{ \\theta}(x^i) )]\n",
    "\\end{equation*}\n",
    "\n",
    "Where h_theta(x) is the prediction from theta transpose x that we ran through the squasher function.  And i is the ith review.  \n",
    "\n",
    "This error function calculates what is known as the cross entropy loss.  Taking the log of the difference is a good way to evaulate model effectiveness.  In the worst case scenario where a real value is actually a 1 [positive], and our model tells us 99.9999% certainty that the prediction is 0, we will have an error that approaches infinity.  Thus severely punishing the model for having the worst guess.\n",
    "\n",
    "Later on when we run the model, we will run batches of update theta functions.  At the core of these functions, we will be subtracting the first derivative with respect to theta_j of our CE function.  Where j indicates the jth parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(predictions, ys):\n",
    "    positive_y_loss = -np.sum(ys * np.log(predictions))\n",
    "    negative_y_loss = -np.sum((1 - ys) * np.log(1 - predictions))\n",
    "    return (positive_y_loss + negative_y_loss) / ys.shape[0] # returns the average CE error\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Want**: to minimize CE.  \n",
    " \n",
    " We can do this using the first derivative of CE.  Defined below.\n",
    " \n",
    " \\begin{equation*}\n",
    " \\frac{\\partial}{\\partial \\theta_{j}}CE = \\sum_{i=1}^m[ (h_{ \\theta}(x^i) - y^i) * x^i_{j} ]\n",
    " \\end{equation*}\n",
    "\n",
    "As the heart of gradient descent, we will run updates across all thetas, where we subract out the derivative with respect to each individual theta.  The derivative is multiplied by a stepping rate (Learning Rate, alpha) to fine tune the steps.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta_{j} = \\theta_{j} - \\alpha * \\frac{\\partial}{\\partial \\theta_{j}}CE = \\theta_{j} - \\alpha * \\sum_{i=1}^m[ (h_{ \\theta}(x^i) - y^i) * x^i_{j} ]\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 4\n",
    "LEARNING_RATE = 0.1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# deriv_wrt_theta is the derivative of our CE error function.\n",
    "\n",
    "def deriv_wrt_theta_j(j, x_is, ys, thetas):\n",
    "    predictions = calc_predictions(x_is, thetas)\n",
    "    return np.sum(np.dot((predictions - ys), x_is[:,j])) / ys.shape[0]\n",
    "\n",
    "def update_thetas(x_is, ys, thetas):   \n",
    "    updated_thetas = [theta - LEARNING_RATE / m * deriv_wrt_theta_j(j, \n",
    "                                                                x_is, \n",
    "                                                                ys, \n",
    "                                                                thetas) for j, theta in enumerate(thetas)]\n",
    "    return np.array(updated_thetas)\n",
    "\n",
    "new_x_test = vectorize_data(x_test)\n",
    "\n",
    "\n",
    "for epoch in range(0, NUM_EPOCHS):\n",
    "    # by updating our thetas every #{BATCH_SIZE} reviews, we can arrive at an answer with lower CE and higher acc\n",
    "    # more efficiently.\n",
    "    for batch_start in range(0, new_x_train.shape[0], BATCH_SIZE):\n",
    "        x_is = new_x_train[batch_start:(batch_start + BATCH_SIZE), :]\n",
    "        ys = y_train[batch_start:(batch_start + BATCH_SIZE)]\n",
    "        thetas = update_thetas(x_is, ys, thetas)\n",
    "        \n",
    "    predictions = calc_predictions(new_x_train, thetas)\n",
    "    acc = accuracy(predictions, y_train)\n",
    "    ce = cross_entropy(predictions, y_train)\n",
    "\n",
    "    test_predictions = calc_predictions(new_x_test, thetas) # Try model wth test data.\n",
    "    test_acc = accuracy(test_predictions, y_test)\n",
    "    \n",
    "    print(\n",
    "        'Epoch #{3} | Error: {0:0.2f} | acc: {1:0.2f} | test acc: {2:0.2f}'.format(ce, acc, test_acc, epoch)\n",
    "    )\n",
    "print(thetas[1:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly we can achieve an 86% accuracy of predicting sentiment solely from using the existance of the top 1_000 words in each review!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
