{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Stochastic Gradient Descent and Multiple Variables\n",
    "##### Boston Housing Prices\n",
    "Data is taken from Keras Datasets.\n",
    "\n",
    "In a series of steps, we can train a simple model to update weights for our multiple parameters.  Over these updates we will minimize the mean squared error between our model's predictions and the 'real answers'.  Finally we will test our model on some testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get our Data.\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the documentation: `Samples contain 13 attributes of houses at different locations around the Boston suburbs in the late 1970s`\n",
    "\n",
    "We have training data from 404 houses.  We have 13 attributes for each house "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is supervised learning seeing as we have the 'correct' answers.  These must be values of the homes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) The Model:\n",
    "\n",
    "We will use an extension of y = mx + b taught in algebra class.  We will add in mx terms for each weight m and parameter x.  I will refer to weights as theta from now on.  \n",
    "\n",
    "So we have\n",
    "\n",
    "\\begin{equation*}\n",
    "h_{ \\theta}(x) = \\theta_{1} * x_{1} + \\theta_{2} * x_{2} + ... \\theta_{j} * x_{j} + b\n",
    "\\end{equation*}\n",
    "\n",
    "We will use our guesses at the weights with our x_training values to generate a prediction for the price of each house.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Initialize Weights and Feature Normalization.\n",
    "\n",
    "Initialize our weights for each x variable.  We can start off guessing all 0s.  \n",
    "Let's also add in 1 weight for the y-intercept b, to take out one step of the model.\n",
    "To do this, we will need to add a column of all 1's to our x_training values.\n",
    "\n",
    "While we are at it, let's normalize features to make it easier for gradient descent to find local minima later on.  We will see that with features and y values on the scale of -1 to 1 that we will converge to an answer much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.zeros(len(x_train[0]) + 1)\n",
    "\n",
    "def add_ones(x):\n",
    "    return np.array([np.insert(arr=row, obj=0, values=1) for row in x])\n",
    "\n",
    "def normalize_features(x):\n",
    "    mean_by_col = x.mean(axis=0)\n",
    "    std_dev_by_col = x.std(axis = 0)\n",
    "    return (x - mean_by_col) / std_dev_by_col\n",
    "\n",
    "def preprocess_data(x):\n",
    "    normalized = normalize_features(x)\n",
    "    return add_ones(normalized)\n",
    "    \n",
    "\n",
    "new_x_train = preprocess_data(x_train)\n",
    "new_y_train = normalize_features(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Calculate Predictions\n",
    "\n",
    "Based on our initial guesses of the weights, we can calculate our first predictions!  Using our model from above in conjunction with converting the intercept as a weight and some matrix math...\n",
    "\n",
    "\\begin{equation*}\n",
    "h_{ \\theta}(x) = \\theta^T * X = \\theta_{0} * x_{0} + \\theta_{1} * x_{1} + \\theta_{2} * x_{2} + ... \\theta_{j} * x_{j}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prediction(thetas, x_is):\n",
    "    return np.dot(x_is, thetas)\n",
    "\n",
    "predictions = calc_prediction(thetas, new_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) The cost function\n",
    "\n",
    "We can expect our model to have a hefty cost since we just guessed all 0's for the weights.  That means we guessed all parameters to have 0 predictive power!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Cost = \\frac{1}{2m} * \\sum_{i=1}^m (h_{ \\theta}(x^i) - y^i)^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.62225272032155"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost_function(ys, predictions):\n",
    "    squared_error = ((predictions - ys) * y_train.std()) ** 2\n",
    "    mean_squared_error = np.mean(squared_error)\n",
    "    return mean_squared_error\n",
    "\n",
    "cost = cost_function(new_y_train, predictions)\n",
    "\n",
    "def std_dev(mse):\n",
    "    return mse ** 0.5\n",
    "\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update Rule:\n",
    "\n",
    "We would like to minimize the cost function.  We can do this by taking the first derivative with respect to theta_j for each weight j, and adjusting our guess for theta_j by that amount times a learning rate.\n",
    " \n",
    " \\begin{equation*}\n",
    " \\frac{\\partial}{\\partial \\theta_{j}}CE = \\sum_{i=1}^m[ (h_{ \\theta}(x^i) - y^i) * x^i_{j} ]\n",
    " \\end{equation*}\n",
    "\n",
    "As the heart of gradient descent, we will run updates across all thetas, where we subract out the derivative with respect to each individual theta.  The derivative is multiplied by a stepping rate (Learning Rate, alpha) to fine tune the steps.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta_{j} = \\theta_{j} - \\alpha * \\frac{\\partial}{\\partial \\theta_{j}}CE = \\theta_{j} - \\alpha * \\sum_{i=1}^m[ (h_{ \\theta}(x^i) - y^i) * x^i_{j} ]\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = .1\n",
    "\n",
    "def deriv_wrt_theta_j(predictions, ys, x_is, j):\n",
    "    m = ys.shape[0]\n",
    "    return np.sum(np.dot((predictions - ys), x_is[:,j])) / m\n",
    "\n",
    "def update_thetas(thetas, ys, x_is):\n",
    "    m = ys.shape[0]\n",
    "    predictions = calc_prediction(thetas, x_is)\n",
    "    new_thetas = [theta - LEARNING_RATE / m * deriv_wrt_theta_j(predictions,\n",
    "                                                    ys,\n",
    "                                                    x_is,\n",
    "                                                    j) for j, theta in enumerate(thetas)]\n",
    "    return np.array(new_thetas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH#0: MSE of 49.98\n",
      "EPOCH#100: MSE of 22.58\n",
      "EPOCH#200: MSE of 22.49\n",
      "EPOCH#300: MSE of 22.49\n",
      "EPOCH#400: MSE of 22.49\n",
      "EPOCH#500: MSE of 22.49\n",
      "[-0.12293773  0.1381039   0.00179547  0.10480701 -0.20931052  0.26430716\n",
      "  0.0651462  -0.36373601  0.33347351]\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 501\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "for epoch in range(0, NUM_EPOCHS):\n",
    "    for batch_start in range(0, new_x_train.shape[0], BATCH_SIZE):\n",
    "        x_batch = new_x_train[batch_start: batch_start + BATCH_SIZE:,]\n",
    "        y_batch = new_y_train[batch_start:batch_start + BATCH_SIZE]\n",
    "        thetas = update_thetas(thetas, y_batch, x_batch)\n",
    "    \n",
    "    predictions = calc_prediction(thetas, new_x_train)\n",
    "    cost = cost_function(new_y_train, predictions)\n",
    "\n",
    "    if (epoch % 100 == 0):\n",
    "        print(\n",
    "            'EPOCH#{0}: MSE of {1:0.2f}'.format(epoch, cost)\n",
    "        )\n",
    "        \n",
    "print(thetas[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that our model is working properly using the normal equation\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta = (X^T * X)^{-1} * X^T * y\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12039218,  0.14709038,  0.0029461 ,  0.10809323, -0.26106711,\n",
       "        0.26049337,  0.02295841, -0.37734568,  0.31613628])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_x = new_x_train.transpose()\n",
    "\n",
    "x_transpose_x = np.dot(transpose_x, new_x_train)\n",
    "inverse = np.linalg.inv(x_transpose_x)\n",
    "m_mult_thetas = inverse.dot(transpose_x).dot(new_y_train)\n",
    "m_mult_thetas[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  We are getting similar values for thetas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
